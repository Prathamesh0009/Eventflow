# Phase 3.4 – Backend Request & Event Flow Execution

> **Goal of this phase**
>
> Describe **exact runtime behavior** of EventFlow:
>
> * How requests move through the backend
> * How events are ingested, processed, and consumed
> * What happens on failure
>
> This phase removes the final gap between *design* and *implementation*.

---

## 1. High-Level Execution Flow

At runtime, EventFlow follows this pattern:

```
Producer → API → Validation → Event Store → Processing → Consumer
                          ↘︎ DLQ (on failure)
```

This flow is intentionally **simple, observable, and recoverable**.

---

## 2. Flow 1: Event Ingestion (Producer → Backend)

### Step-by-Step Execution

1. Producer sends HTTP request

   * `POST /api/v1/events`
   * API Key in header

2. API Gateway / Controller

   * Authenticates API key
   * Rate limits request

3. Validation Layer

   * Validate JSON payload
   * Validate schema against `event_stream.schema`

4. Event Creation

   * Generate `event_id`
   * Attach metadata:

     * timestamp
     * producer_id
     * trace_id

5. Persistence

   * Insert event into `events` table (append-only)

6. Acknowledge Producer

   * HTTP `202 Accepted`
   * Event is now durable

---

## 3. Flow 2: Internal Event Processing

### Purpose

Decouple ingestion from heavy logic.

### Execution Steps

1. Event picked from internal queue

2. Processing rules evaluated

3. One of the following actions occurs:

   * Pass-through
   * Transform payload
   * Filter (drop)

4. Successful events marked as processed

> ⚠️ Processing is **async** — producer is never blocked

---

## 4. Flow 3: Event Consumption (Backend → Consumer)

### Step-by-Step Execution

1. Consumer polls events

   * `GET /api/v1/streams/{id}/events`
   * Consumer group identified

2. Offset Lookup

   * Fetch last offset from `consumer_offsets`

3. Event Selection

   * Fetch next batch of events

4. Event Delivery

   * Return events to consumer

5. Offset Commit

   * Consumer confirms processing
   * Offset updated

---

## 5. Offset Management Logic

* Offsets stored **per consumer group**
* Offset update is atomic
* No auto-commit (explicit commit only)

This ensures:

* No data loss
* Replay capability
* Exactly-once *processing intent*

---

## 6. Flow 4: Failure Handling & Retries

### Failure Scenarios

| Failure Point    | Action             |
| ---------------- | ------------------ |
| Validation       | Reject request     |
| DB write         | Retry insert       |
| Processing error | Retry with backoff |
| Repeated failure | Move to DLQ        |

---

## 7. Dead Letter Queue (DLQ) Flow

1. Event fails processing N times
2. Event copied to `dead_letter_events`
3. Failure reason stored
4. Alert generated

DLQ allows:

* Manual inspection
* Reprocessing later
* Zero silent failures

---

## 8. Idempotency & Safety

* `event_id` guarantees uniqueness
* Duplicate events safely ignored
* Consumers protected from double delivery

---

## 9. Observability During Execution

### Logged at Each Step

* Request received
* Event persisted
* Processing success/failure
* Offset commit

### Metrics Collected

* Ingestion rate
* Processing latency
* Consumer lag
* DLQ size

---

## 10. End-to-End Sequence (Textual Diagram)

```
POST /events
 → validate
 → store event
 → enqueue
 → process
 → available for consumers
 → consume
 → commit offset
```