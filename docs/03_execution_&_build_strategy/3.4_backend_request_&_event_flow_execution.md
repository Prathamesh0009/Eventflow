# Phase 3.4 – Backend Request & Event Flow Execution

> **Goal of this phase**
>
> Describe **exact runtime behavior** of EventFlow:
>
> * How requests move through the backend
> * How events are ingested, processed, and consumed
> * What happens on failure
>
> This phase removes the final gap between *design* and *implementation*.

---

## 1. High-Level Execution Flow

At runtime, EventFlow follows this pattern:

```
Producer → API → Validation → Event Store → Processing → Consumer
                          ↘︎ DLQ (on failure)
```

This flow is intentionally **simple, observable, and recoverable**.

---

## 2. Flow 1: Event Ingestion (Producer → Backend)

### Step-by-Step Execution

1. Producer sends HTTP request

   * `POST /api/v1/streams/{stream_id}/events`
   * API Key in header (`X-API-Key`)
   * Stream ID in path

2. API Gateway / Controller

   * Authenticates API key → resolves `org_id` and `api_key_id`
   * Validates stream exists and belongs to organization
   * Rate limits request (per API key)

3. Validation Layer

   * Validate JSON payload structure
   * Validate schema against `event_streams.schema` (if defined)
   * Validate required fields in payload

4. Event Creation

   * Generate `event_id` (UUID v4)
   * Build metadata object:
     * `event_type` (from request)
     * `event_version` (from request)
     * `occurred_at` (from request or current time)
     * `source` (from request or default)
     * `api_key_id` (from authenticated key)
     * `context` (trace_id, correlation_id from request)
   * Store payload as-is (JSONB)

5. Persistence

   * Insert event into `events` table (append-only):
     * `id` = generated event_id
     * `stream_id` = from path parameter
     * `payload` = request payload (JSONB)
     * `metadata` = complete metadata object (JSONB)
     * `created_at` = current timestamp
   * Transaction ensures atomicity

6. Acknowledge Producer

   * HTTP `202 Accepted`
   * Response body: `{ "event_id": "...", "status": "ACCEPTED", "received_at": "..." }`
   * Event is now durable and immutable

---

## 3. Flow 2: Internal Event Processing

### Purpose

Decouple ingestion from heavy logic.

### Execution Steps

1. Event picked from internal queue

2. Processing rules evaluated

3. One of the following actions occurs:

   * Pass-through
   * Transform payload
   * Filter (drop)

4. Successful events marked as processed

> ⚠️ Processing is **async** — producer is never blocked

---

## 4. Flow 3: Event Consumption (Backend → Consumer)

### Step-by-Step Execution

1. Consumer polls events

   * `GET /api/v1/streams/{stream_id}/consumer-groups/{group_id}/events`
   * Consumer group identified via path
   * Optional query params: `limit`, `timeout` (long polling)

2. Offset Lookup

   * Fetch last offset from `consumer_offsets` table:
     * Query: `SELECT last_event_id FROM consumer_offsets WHERE consumer_group_id = ?`
   * If no offset exists, start from beginning of stream

3. Event Selection

   * Fetch next batch of events:
     * Query: `SELECT * FROM events WHERE stream_id = ? AND id > ? ORDER BY created_at ASC LIMIT ?`
   * Events ordered by `created_at` (time-ordered)

4. Event Delivery

   * Return events to consumer:
     * Response includes `events` array and `next_offset` (last event ID)
   * Consumer uses `next_offset` for next poll

5. Offset Commit

   * Consumer confirms processing:
     * `POST /api/v1/streams/{stream_id}/consumer-groups/{group_id}/offsets`
     * Body: `{ "last_event_id": "uuid" }`
   * Offset updated atomically:
     * `UPSERT INTO consumer_offsets (consumer_group_id, last_event_id, updated_at) VALUES (?, ?, NOW())`

---

## 5. Offset Management Logic

* Offsets stored **per consumer group**
* Offset update is atomic
* No auto-commit (explicit commit only)

This ensures:

* No data loss
* Replay capability
* Exactly-once *processing intent*

---

## 6. Flow 4: Failure Handling & Retries

### Failure Scenarios

| Failure Point    | Action             |
| ---------------- | ------------------ |
| Validation       | Reject request     |
| DB write         | Retry insert       |
| Processing error | Retry with backoff |
| Repeated failure | Move to DLQ        |

---

## 7. Dead Letter Queue (DLQ) Flow

1. Event fails processing N times
2. Event copied to `dead_letter_events`
3. Failure reason stored
4. Alert generated

DLQ allows:

* Manual inspection
* Reprocessing later
* Zero silent failures

---

## 8. Idempotency & Safety

* `event_id` guarantees uniqueness
* Duplicate events safely ignored
* Consumers protected from double delivery

---

## 9. Observability During Execution

### Logged at Each Step

* Request received
* Event persisted
* Processing success/failure
* Offset commit

### Metrics Collected

* Ingestion rate
* Processing latency
* Consumer lag
* DLQ size

---

## 10. End-to-End Sequence (Textual Diagram)

```
POST /events
 → validate
 → store event
 → enqueue
 → process
 → available for consumers
 → consume
 → commit offset
```